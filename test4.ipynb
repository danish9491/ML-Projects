{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone Fracture Classification Using Custom CNN in PyTorch\n",
    "\n",
    "In this project, we aim to build a convolutional neural network (CNN) to classify bone fracture images as either fractured or not fractured. The CNN will be implemented using PyTorch, with image data preprocessing, augmentation, model training, and evaluation steps documented below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "In this step, we import necessary libraries required for building the model, handling image data, and performing analysis. \n",
    "\n",
    "- `torch` and `torchvision`: For model building and image processing.\n",
    "- `matplotlib`: For plotting results.\n",
    "- `sklearn.metrics`: For model evaluation using metrics like classification report and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess Data\n",
    "\n",
    "We define the directory paths for our training, validation, and test datasets. Then, we define transformations to apply to the images, including resizing, normalization, and data augmentation techniques (like random flipping and rotations for the training set). \n",
    "\n",
    "Data augmentation helps improve the model's ability to generalize by introducing variability in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directories\n",
    "train_dir = r\"D:\\College Notes\\5th Sem\\CS307 Machine Learning 4\\Bone_Fracture_Binary_Classification\\train\"\n",
    "val_dir = r\"D:\\College Notes\\5th Sem\\CS307 Machine Learning 4\\Bone_Fracture_Binary_Classification\\val\"\n",
    "test_dir = r\"D:\\College Notes\\5th Sem\\CS307 Machine Learning 4\\Bone_Fracture_Binary_Classification\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data augmentation for training and validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "        transforms.RandomHorizontalFlip(),  # Random horizontal flip for augmentation\n",
    "        transforms.RandomRotation(10),  # Randomly rotate images by 10 degrees\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random zoom\n",
    "        transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transforms['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Loaders\n",
    "\n",
    "Here we create PyTorch `DataLoader` objects to efficiently load and batch our datasets. We also shuffle the training data to introduce randomness in training and provide information on the number of samples in each dataset.\n",
    "\n",
    "- **Batch Size**: We use a batch size of 32 for training, validation, and testing.\n",
    "- **Shuffling**: The training data is shuffled to ensure that each mini-batch is different from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with a defined batch size\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9246\n",
      "Number of validation samples: 829\n",
      "Number of test samples: 506\n"
     ]
    }
   ],
   "source": [
    "# Display dataset sizes and class names\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fractured', 'not fractured']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = os.listdir(train_dir)\n",
    "print(f\"Classes found: {len(classes)}\")\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Custom CNN Model\n",
    "\n",
    "In this step, we define our custom CNN architecture using PyTorch. The model includes the following components:\n",
    "\n",
    "- **Convolutional Layers**: Three convolutional layers to extract features from the input images, with increasing numbers of filters.\n",
    "- **Batch Normalization**: To stabilize and accelerate training.\n",
    "- **Max Pooling**: After each convolutional layer, we apply max pooling to down-sample the feature maps.\n",
    "- **Fully Connected Layers**: Two fully connected layers to learn the higher-level representations and output the final prediction.\n",
    "\n",
    "The final layer uses a **Sigmoid activation function** for binary classification (fractured vs. not fractured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN architecture with Batch Normalization and Dropout\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Define convolutional layers with batch normalization\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input: RGB image\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling layer\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)  # Adjust based on image size after pooling\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(256, 1)  # Binary classification output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through convolutional layers with batch normalization and pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 28 * 28)  # Flatten the tensor for the fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer with ReLU activation\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Device, Loss Function, and Optimizer\n",
    "\n",
    "We move the model to the GPU (if available) to leverage faster training. We define:\n",
    "\n",
    "- **Loss Function**: `BCELoss` (Binary Cross Entropy Loss) is used because this is a binary classification task.\n",
    "- **Optimizer**: `Adam` optimizer is used for weight updates, known for its adaptive learning rate and good convergence properties.\n",
    "- **Learning Rate Scheduler**: A step-based scheduler that reduces the learning rate after every 5 epochs is applied to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device for GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model and move it to the specified device\n",
    "model = CustomCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # For binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model\n",
    "\n",
    "This section contains the core training loop for the CNN. Each epoch consists of:\n",
    "\n",
    "- **Forward Pass**: Passing the input images through the model.\n",
    "- **Loss Calculation**: Calculating the binary cross entropy loss.\n",
    "- **Backpropagation**: Computing gradients and updating weights using the optimizer.\n",
    "- **Validation**: After each epoch, the model is evaluated on the validation set to monitor progress and prevent overfitting.\n",
    "\n",
    "We print the training and validation loss after every epoch to track performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop function with learning rate scheduling\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs.view(-1), labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.float().to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.view(-1), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daans\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 49.6550, Val Loss: 40.5048\n",
      "Epoch [2/10], Train Loss: 49.8162, Val Loss: 40.5048\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "\n",
    "After training, we evaluate the model on the test dataset. The following metrics are used for evaluation:\n",
    "\n",
    "- **Classification Report**: Shows precision, recall, and F1-score for each class.\n",
    "- **Confusion Matrix**: Provides insights into the number of true positives, true negatives, false positives, and false negatives.\n",
    "- **AUC-ROC Score**: This metric evaluates the ability of the model to distinguish between classes, which is especially useful in binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set and calculate metrics\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = (outputs.view(-1) > 0.5).float()  # Convert probabilities to binary predictions\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # AUC-ROC Score\n",
    "    auc_score = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"AUC-ROC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Model Performance\n",
    "\n",
    "Here, we analyze the model's performance based on the test evaluation metrics. We discuss:\n",
    "\n",
    "- The confusion matrix to identify any potential issues with class imbalance.\n",
    "- The AUC-ROC score to measure how well the model can separate the fractured and non-fractured images.\n",
    "- The classification report to examine the precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Next Steps and Improvements\n",
    "\n",
    "- **Hyperparameter Tuning**: Experimenting with different batch sizes, learning rates, and optimizers.\n",
    "- **Early Stopping**: Implementing early stopping to prevent overfitting during training.\n",
    "- **Data Augmentation**: Adding more aggressive augmentation techniques like random brightness or contrast adjustments.\n",
    "- **Transfer Learning**: Exploring transfer learning using pre-trained models like ResNet to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
